import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import streamlit as st
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Dropout, Attention, BatchNormalization
from tensorflow.keras.optimizers import Nadam, Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.utils import to_categorical
import pandas as pd # Adicionado para visualizaÃ§Ã£o da tabela

from collections import Counter, deque
import random
import logging
import json

# =========================
# Utils Streamlit
# =========================
def rerun():
Â  Â  # Mantido por compatibilidade (nÃ£o usado diretamente)
Â  Â  raise st.script_runner.RerunException(st.script_request_queue.RerunData())

logging.basicConfig(filename='roleta.log', filemode='a',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  format='%(asctime)s - %(levelname)s - %(message)s',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  level=logging.INFO)
logger = logging.getLogger()

# --- SESSION STATE INIT ---
if 'history' not in st.session_state:
Â  Â  st.session_state.history = []
if 'model' not in st.session_state:
Â  Â  st.session_state.model = None
if 'dqn_agent' not in st.session_state:
Â  Â  st.session_state.dqn_agent = None
if 'stats' not in st.session_state:
Â  Â  st.session_state.stats = {'wins': 0, 'bets': 0, 'streak': 0, 'max_streak': 0, 'profit': 0.0}
if 'last_input' not in st.session_state:
Â  Â  st.session_state.last_input = None
if 'step_count' not in st.session_state:
Â  Â  st.session_state.step_count = 0
if 'prev_state' not in st.session_state:
Â  Â  st.session_state.prev_state = None
if 'prev_action' not in st.session_state:
Â  Â  st.session_state.prev_action = None
if 'prev_actions' not in st.session_state:
Â  Â  st.session_state.prev_actions = None
if 'input_bulk' not in st.session_state:
Â  Â  st.session_state.input_bulk = ""
if 'clear_input_bulk' not in st.session_state:
Â  Â  st.session_state.clear_input_bulk = False
# =========================================================================
# NOVOS ESTADOS PARA A TABELA DE NÃšMEROS QUE SE PUXAM (CO-OCORRÃŠNCIA)
# =========================================================================
if 'pull_table' not in st.session_state:
Â  Â  # A tabela de co-ocorrÃªncia: pull_table[num_anterior][num_atual] = contagem
Â  Â  st.session_state.pull_table = {str(i): {str(j): 0 for j in range(37)} for i in range(37)}

# --- CONSTANTS ---
NUM_TOTAL = 37
SEQUENCE_LEN = 20
BET_AMOUNT = 1.0

# Replay/treino DQN
TARGET_UPDATE_FREQ = 50
REPLAY_BATCH = 100
REPLAY_SIZE = 5000
DQN_TRAIN_EVERY = 5
DQN_LEARNING_RATE = 1e-3
DQN_GAMMA = 0.95

# Recompensa (shaping)
REWARD_EXACT = 35.0
REWARD_NEIGHBOR = 5.0
REWARD_COLOR = 0.0
REWARD_DOZEN = 0.0
REWARD_LOSS = -15.0
NEIGHBOR_RADIUS_FOR_REWARD = 3

# Treino LSTM incremental
LSTM_RECENT_WINDOWS = 400
LSTM_BATCH_SAMPLES = 128
LSTM_EPOCHS_PER_STEP = 2
LSTM_BATCH_SIZE = 32

# HiperparÃ¢metros DQN (exploraÃ§Ã£o)
EPSILON_START = 1.0
EPSILON_MIN = 0.05
EPSILON_DECAY = 0.992

# wheel order (posiÃ§Ã£o fÃ­sica na roda)
WHEEL_ORDER = [0,32,15,19,4,21,2,25,17,34,6,27,13,36,11,30,8,23,10,
Â  Â  Â  Â  Â  Â  Â  Â 5,24,16,33,1,20,14,31,9,22,18,29,7,28,12,35,3,26]
WHEEL_DISTANCE = [[min(abs(i-j), 37-abs(i-j)) for j in range(37)] for i in range(37)]

# red numbers for European wheel (standard)
RED_NUMBERS = {1,3,5,7,9,12,14,16,18,19,21,23,25,27,30,32,34,36}

# Definindo as principais regiÃµes da roleta
# Baseado na ordem fÃ­sica da roda europeia
REGIONS = {
Â  Â  "zero_spiel": {0, 32, 15, 19, 4, 21, 2, 25}, # 8 nÃºmeros
Â  Â  "voisins_du_zero": {0, 2, 3, 4, 7, 12, 15, 18, 19, 21, 22, 25, 26, 28, 29, 32, 35, 36}, # 17 nÃºmeros (Vizinhos)
Â  Â  "tiers_du_cylindre": {27, 13, 36, 11, 30, 8, 23, 10, 5, 24, 16, 33}, # 12 nÃºmeros (TerÃ§o)
Â  Â  "orphelins": {1, 6, 9, 14, 17, 20, 31, 34}, # 8 nÃºmeros (Ã“rfÃ£os)
Â  Â  "juego_del_cero": {12, 15, 32, 19, 26, 3, 35, 0},
Â  Â  "petite_serie": {5, 8, 10, 11, 13, 16, 23, 24, 27, 30, 33, 36}
}

# Mapeia cada nÃºmero para uma (ou mais) regiÃ£o
NUMBER_TO_REGION = {n: [] for n in range(NUM_TOTAL)}
for region_name, numbers in REGIONS.items():
Â  Â  for num in numbers:
Â  Â  Â  Â  NUMBER_TO_REGION[num].append(region_name)

# --- AUX FUNCTIONS ---
def number_to_color(n):
Â  Â  if n == 0:
Â  Â  Â  Â  return 0 # zero
Â  Â  return 1 if n in RED_NUMBERS else 2 # 1=red,2=black

def number_to_dozen(n):
Â  Â  if n == 0:
Â  Â  Â  Â  return 0 # zero
Â  Â  if 1 <= n <= 12:
Â  Â  Â  Â  return 1
Â  Â  if 13 <= n <= 24:
Â  Â  Â  Â  return 2
Â  Â  return 3

def number_to_region(n):
Â  Â  """Retorna o Ã­ndice da primeira regiÃ£o que o nÃºmero pertence.
Â  Â  Retorna -1 se nÃ£o pertencer a nenhuma regiÃ£o definida."""
Â  Â  for i, region_name in enumerate(REGIONS):
Â  Â  Â  Â  if n in REGIONS[region_name]:
Â  Â  Â  Â  Â  Â  return i
Â  Â  return -1

def get_advanced_features(sequence):
Â  Â  if sequence is None or len(sequence) < 2:
Â  Â  Â  Â  return [0.0]*8
Â  Â  seq = np.array(sequence)
Â  Â  mean = np.mean(seq)
Â  Â  std = np.std(seq)
Â  Â  last = int(sequence[-1])
Â  Â  second_last = int(sequence[-2])
Â  Â  if last in WHEEL_ORDER and second_last in WHEEL_ORDER:
Â  Â  Â  Â  last_pos = WHEEL_ORDER.index(last)
Â  Â  Â  Â  second_last_pos = WHEEL_ORDER.index(second_last)
Â  Â  Â  Â  wheel_speed = (last_pos - second_last_pos) % 37
Â  Â  Â  Â  if len(sequence) > 2 and sequence[-3] in WHEEL_ORDER:
Â  Â  Â  Â  Â  Â  third_pos = WHEEL_ORDER.index(sequence[-3])
Â  Â  Â  Â  Â  Â  prev_speed = (second_last_pos - third_pos) % 37
Â  Â  Â  Â  Â  Â  deceleration = abs(wheel_speed - prev_speed)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  deceleration = 0.0
Â  Â  else:
Â  Â  Â  Â  wheel_speed = 0.0
Â  Â  Â  Â  deceleration = 0.0
Â  Â  freq = Counter(sequence)
Â  Â  hot_number = max(freq.values()) if freq else 1
Â  Â  cold_number = min(freq.values()) if freq else 0
Â  Â  return [
Â  Â  Â  Â  mean / 36.0,
Â  Â  Â  Â  std / 18.0,
Â  Â  Â  Â  wheel_speed / 36.0,
Â  Â  Â  Â  deceleration / 36.0,
Â  Â  Â  Â  freq.get(last, 0) / hot_number if hot_number > 0 else 0.0,
Â  Â  Â  Â  (hot_number - cold_number) / len(sequence) if len(sequence) > 0 else 0.0,
Â  Â  Â  Â  len(freq) / len(sequence) if len(sequence) > 0 else 0.0,
Â  Â  Â  Â  1.0 if last == second_last else 0.0
Â  Â  ]

def sequence_to_one_hot(sequence):
Â  Â  """
Â  Â  Retorna array (SEQUENCE_LEN, NUM_TOTAL) com one-hot da posiÃ§Ã£o na roda.
Â  Â  Padding (quando falta histÃ³rico) -> vetor zeros.
Â  Â  """
Â  Â  seq = list(sequence[-SEQUENCE_LEN:]) if sequence else []
Â  Â  pad = [-1] * max(0, (SEQUENCE_LEN - len(seq)))
Â  Â  seq_padded = pad + seq
Â  Â  one_hot_seq = []
Â  Â  for x in seq_padded:
Â  Â  Â  Â  if x in WHEEL_ORDER:
Â  Â  Â  Â  Â  Â  pos = WHEEL_ORDER.index(x)
Â  Â  Â  Â  Â  Â  one_hot_seq.append(to_categorical(pos, NUM_TOTAL))
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  one_hot_seq.append(np.zeros(NUM_TOTAL))
Â  Â  return np.array(one_hot_seq)

def sequence_to_state(sequence, model=None):
Â  Â  """
Â  Â  Retorna estado 1D para o DQN: [one_hot_seq.flatten(), features, num_probs, color_probs, dozen_probs, age_vector, new_features]
Â  Â  """
Â  Â  seq_slice = sequence[-SEQUENCE_LEN:] if sequence else []

Â  Â  # 1. Base State Features (mantidas)
Â  Â  one_hot_seq = sequence_to_one_hot(seq_slice)
Â  Â  features = get_advanced_features(seq_slice)
Â  Â  
Â  Â  # 2. LSTM Prediction Probabilities (mantidas)
Â  Â  num_probs = np.zeros(NUM_TOTAL)
Â  Â  color_probs = np.zeros(3)
Â  Â  dozen_probs = np.zeros(4)
Â  Â  if model is not None and len(sequence) >= SEQUENCE_LEN:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  seq_arr = np.expand_dims(one_hot_seq, axis=0)
Â  Â  Â  Â  Â  Â  feat_arr = np.array([features])
Â  Â  Â  Â  Â  Â  raw = model.predict([seq_arr, feat_arr], verbose=0)
Â  Â  Â  Â  Â  Â  if isinstance(raw, list) and len(raw) == 3:
Â  Â  Â  Â  Â  Â  Â  Â  num_probs = np.array(raw[0][0])
Â  Â  Â  Â  Â  Â  Â  Â  color_probs = np.array(raw[1][0])
Â  Â  Â  Â  Â  Â  Â  Â  dozen_probs = np.array(raw[2][0])
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  # Adicionando verificaÃ§Ã£o de forma
Â  Â  Â  Â  Â  Â  Â  Â  if num_probs.shape != (NUM_TOTAL,):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  num_probs = np.zeros(NUM_TOTAL)
Â  Â  Â  Â  Â  Â  Â  Â  if color_probs.shape != (3,):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  color_probs = np.zeros(3)
Â  Â  Â  Â  Â  Â  Â  Â  if dozen_probs.shape != (4,):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dozen_probs = np.zeros(4)
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  # Em caso de erro, os vetores de probabilidade permanecem zeros
Â  Â  Â  Â  Â  Â  pass

Â  Â  # 3. Vetor de Idade (mantido e normalizado)
Â  Â  age_vector = [0] * NUM_TOTAL
Â  Â  last_seen = {num: i for i, num in enumerate(sequence)}
Â  Â  for num in range(NUM_TOTAL):
Â  Â  Â  Â  if num in last_seen:
Â  Â  Â  Â  Â  Â  age_vector[num] = len(sequence) - 1 - last_seen[num]
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  age_vector[num] = len(sequence)
Â  Â  max_age = max(age_vector) if age_vector else 1
Â  Â  age_vector = [age / max(1, max_age) for age in age_vector]
Â  Â  age_vector = np.array(age_vector)

Â  Â  # ==================================
Â  Â  # === NOVAS FEATURES ADICIONADAS ===
Â  Â  # ==================================
Â  Â  
Â  Â  # Nova Feature 1: Run Length Encoding (Streaks e AlternÃ¢ncias)
Â  Â  # Apenas para o Ãºltimo nÃºmero, cor e dÃºzia
Â  Â  last_run_len_num = 0
Â  Â  if len(sequence) >= 2:
Â  Â  Â  Â  for i in range(1, len(sequence)):
Â  Â  Â  Â  Â  Â  if sequence[-i] == sequence[-1]:
Â  Â  Â  Â  Â  Â  Â  Â  last_run_len_num = i
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  last_run_len_color = 0
Â  Â  if len(sequence) >= 2:
Â  Â  Â  Â  last_color = number_to_color(sequence[-1])
Â  Â  Â  Â  for i in range(1, len(sequence)):
Â  Â  Â  Â  Â  Â  if number_to_color(sequence[-i]) == last_color:
Â  Â  Â  Â  Â  Â  Â  Â  last_run_len_color = i
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  last_run_len_dozen = 0
Â  Â  if len(sequence) >= 2:
Â  Â  Â  Â  last_dozen = number_to_dozen(sequence[-1])
Â  Â  Â  Â  for i in range(1, len(sequence)):
Â  Â  Â  Â  Â  Â  if number_to_dozen(sequence[-i]) == last_dozen:
Â  Â  Â  Â  Â  Â  Â  Â  last_run_len_dozen = i
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  run_length_features = np.array([last_run_len_num / SEQUENCE_LEN, last_run_len_color / SEQUENCE_LEN, last_run_len_dozen / SEQUENCE_LEN])
Â  Â  
Â  Â  # Nova Feature 2: Grupos do Ãšltimo NÃºmero (One-hot para Cor e DÃºzia)
Â  Â  last_num = sequence[-1] if sequence else -1
Â  Â  last_color_one_hot = to_categorical(number_to_color(last_num), 3) if last_num in range(NUM_TOTAL) else np.zeros(3)
Â  Â  last_dozen_one_hot = to_categorical(number_to_dozen(last_num), 4) if last_num in range(NUM_TOTAL) else np.zeros(4)
Â  Â  
Â  Â  # Nova Feature 3: ProporÃ§Ãµes de Par/Ãmpar e Alto/Baixo
Â  Â  recent_seq = seq_slice
Â  Â  even_count = sum(1 for n in recent_seq if n % 2 == 0 and n != 0)
Â  Â  odd_count = sum(1 for n in recent_seq if n % 2 != 0)
Â  Â  high_count = sum(1 for n in recent_seq if n >= 19 and n <= 36)
Â  Â  low_count = sum(1 for n in recent_seq if n >= 1 and n <= 18)
Â  Â  total_non_zero = even_count + odd_count
Â  Â  
Â  Â  even_odd_ratio = even_count / max(1, total_non_zero)
Â  Â  high_low_ratio = high_count / max(1, high_count + low_count)
Â  Â  
Â  Â  group_ratio_features = np.array([even_odd_ratio, high_low_ratio])
Â  Â  
Â  Â  # NOVAS FEATURES DE REGIÃ•ES DA RODA
Â  Â  num_regions = len(REGIONS)
Â  Â  last_region_one_hot = np.zeros(num_regions)
Â  Â  region_proportions = np.zeros(num_regions)
Â  Â  region_streak = 0
Â  Â  
Â  Â  # Calcula proporÃ§Ãµes de regiÃµes e streak
Â  Â  if len(recent_seq) > 0:
Â  Â  Â  Â  region_counts = Counter(number_to_region(n) for n in recent_seq)
Â  Â  Â  Â  for i in range(num_regions):
Â  Â  Â  Â  Â  Â  region_proportions[i] = region_counts.get(i, 0) / len(recent_seq)

Â  Â  Â  Â  last_region = number_to_region(recent_seq[-1])
Â  Â  Â  Â  if last_region != -1:
Â  Â  Â  Â  Â  Â  last_region_one_hot[last_region] = 1
Â  Â  Â  Â  Â  Â  for i in range(1, len(recent_seq) + 1):
Â  Â  Â  Â  Â  Â  Â  Â  if number_to_region(recent_seq[-i]) == last_region:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  region_streak += 1
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  
Â  Â  region_streak_norm = region_streak / SEQUENCE_LEN
Â  Â  
Â  Â  # Combine todos os vetores para formar o estado final
Â  Â  state = np.concatenate([
Â  Â  Â  Â  one_hot_seq.flatten(),
Â  Â  Â  Â  np.array(features),
Â  Â  Â  Â  num_probs,
Â  Â  Â  Â  color_probs,
Â  Â  Â  Â  dozen_probs,
Â  Â  Â  Â  age_vector,
Â  Â  Â  Â  run_length_features,
Â  Â  Â  Â  last_color_one_hot,
Â  Â  Â  Â  last_dozen_one_hot,
Â  Â  Â  Â  group_ratio_features,
Â  Â  Â  Â  last_region_one_hot,
Â  Â  Â  Â  region_proportions,
Â  Â  Â  Â  np.array([region_streak_norm])
Â  Â  ]).astype(np.float32)

Â  Â  return state

# =========================
# MODELO LSTM â€“ ARQUITETURA REFINADA
# =========================
def build_deep_learning_model(seq_len=SEQUENCE_LEN, num_total=NUM_TOTAL):
Â  Â  """
Â  Â  LSTM multi-output com maior profundidade + atenÃ§Ã£o:
Â  Â  Â  - saÃ­da 1: probabilidade para cada nÃºmero (37)
Â  Â  Â  - saÃ­da 2: probabilidade para cor (3) -> zero/red/black
Â  Â  Â  - saÃ­da 3: probabilidade para dÃºzia (4) -> zero/d1/d2/d3
Â  Â  """
Â  Â  seq_input = Input(shape=(seq_len, num_total), name='sequence_input')

Â  Â  # Pilha LSTM mais profunda
Â  Â  x = LSTM(128, return_sequences=True, kernel_regularizer=l2(1e-4))(seq_input)
Â  Â  x = BatchNormalization()(x)
Â  Â  x = Dropout(0.3)(x)

Â  Â  x = LSTM(96, return_sequences=True, kernel_regularizer=l2(1e-4))(x)
Â  Â  x = BatchNormalization()(x)
Â  Â  x = Dropout(0.25)(x)

Â  Â  # Self-Attention simples
Â  Â  x_att = Attention(name="self_attention")([x, x])

Â  Â  # Mais uma LSTM para sintetizar apÃ³s atenÃ§Ã£o
Â  Â  x = LSTM(64, return_sequences=False)(x_att)
Â  Â  x = BatchNormalization()(x)
Â  Â  x = Dropout(0.25)(x)

Â  Â  # Features adicionais
Â  Â  feat_input = Input(shape=(8,), name='features_input')
Â  Â  dense_feat = Dense(48, activation='swish')(feat_input)
Â  Â  dense_feat = BatchNormalization()(dense_feat)
Â  Â  dense_feat = Dropout(0.2)(dense_feat)

Â  Â  # Combina LSTM + features
Â  Â  combined = Concatenate()([x, dense_feat])
Â  Â  dense = Dense(160, activation='swish')(combined)
Â  Â  dense = BatchNormalization()(dense)
Â  Â  dense = Dropout(0.3)(dense)

Â  Â  out_num = Dense(num_total, activation='softmax', name='num_out')(dense)
Â  Â  out_color = Dense(3, activation='softmax', name='color_out')(dense) # zero/red/black
Â  Â  out_dozen = Dense(4, activation='softmax', name='dozen_out')(dense) # zero,d1,d2,d3

Â  Â  model = Model(inputs=[seq_input, feat_input], outputs=[out_num, out_color, out_dozen])
Â  Â  optimizer = Nadam(learning_rate=4e-4)
Â  Â  model.compile(optimizer=optimizer,
Â  Â  Â  Â  Â  Â  Â  Â  Â  loss={'num_out': 'categorical_crossentropy',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'color_out': 'categorical_crossentropy',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'dozen_out': 'categorical_crossentropy'},
Â  Â  Â  Â  Â  Â  Â  Â  Â  loss_weights={'num_out': 1.0, 'color_out': 0.35, 'dozen_out': 0.35},
Â  Â  Â  Â  Â  Â  Â  Â  Â  metrics={'num_out': 'accuracy'})
Â  Â  return model

# =========================
# DQN Agent
# =========================
class DQNAgent:
Â  Â  def __init__(self, state_size, action_size, lr=DQN_LEARNING_RATE, gamma=DQN_GAMMA, replay_size=REPLAY_SIZE):
Â  Â  Â  Â  self.state_size = int(state_size)
Â  Â  Â  Â  self.action_size = action_size
Â  Â  Â  Â  self.memory = deque(maxlen=replay_size)
Â  Â  Â  Â  self.gamma = gamma
Â  Â  Â  Â  self.epsilon = EPSILON_START
Â  Â  Â  Â  self.epsilon_min = EPSILON_MIN
Â  Â  Â  Â  self.epsilon_decay = EPSILON_DECAY
Â  Â  Â  Â  self.learning_rate = lr
Â  Â  Â  Â  self.model = self._build_model()
Â  Â  Â  Â  self.target_model = self._build_model()
Â  Â  Â  Â  self.update_target()
Â  Â  Â  Â  self.train_step = 0

Â  Â  def _build_model(self):
Â  Â  Â  Â  model = tf.keras.Sequential([
Â  Â  Â  Â  Â  Â  Dense(320, activation='relu', input_shape=(self.state_size,)),
Â  Â  Â  Â  Â  Â  BatchNormalization(),
Â  Â  Â  Â  Â  Â  Dropout(0.3),
Â  Â  Â  Â  Â  Â  Dense(160, activation='relu'),
Â  Â  Â  Â  Â  Â  Dense(self.action_size, activation='linear')
Â  Â  Â  Â  ])
Â  Â  Â  Â  model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss='mse')
Â  Â  Â  Â  return model

Â  Â  def update_target(self):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  self.target_model.set_weights(self.model.get_weights())
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  pass

Â  Â  def remember(self, state, action, reward, next_state, done):
Â  Â  Â  Â  if state is None or next_state is None:
Â  Â  Â  Â  Â  Â  return
Â  Â  Â  Â  self.memory.append((state, action, reward, next_state, done))

Â  Â  def act_top_k(self, state, k=3, use_epsilon=True):
Â  Â  Â  Â  if state is None or len(state) == 0:
Â  Â  Â  Â  Â  Â  return random.sample(range(self.action_size), k)
Â  Â  Â  Â  # ExploraÃ§Ã£o
Â  Â  Â  Â  if use_epsilon and np.random.rand() <= self.epsilon:
Â  Â  Â  Â  Â  Â  return random.sample(range(self.action_size), k)
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  q_values = self.model.predict(np.array([state]), verbose=0)[0]
Â  Â  Â  Â  Â  Â  top_k_actions = np.argsort(q_values)[-k:][::-1]
Â  Â  Â  Â  Â  Â  return top_k_actions.tolist()
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  return random.sample(range(self.action_size), k)

Â  Â  def act(self, state, use_epsilon=True):
Â  Â  Â  Â  if state is None or len(state) == 0:
Â  Â  Â  Â  Â  Â  return random.randrange(self.action_size)
Â  Â  Â  Â  if use_epsilon and np.random.rand() <= self.epsilon:
Â  Â  Â  Â  Â  Â  return random.randrange(self.action_size)
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  q_values = self.model.predict(np.array([state]), verbose=0)[0]
Â  Â  Â  Â  Â  Â  return int(np.argmax(q_values))
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  return random.randrange(self.action_size)

Â  Â  def replay(self, batch_size=REPLAY_BATCH):
Â  Â  Â  Â  if len(self.memory) < batch_size:
Â  Â  Â  Â  Â  Â  return
Â  Â  Â  Â  
Â  Â  Â  Â  batch = random.sample(self.memory, batch_size)
Â  Â  Â  Â  
Â  Â  Â  Â  # Preparar dados para o modelo (o estado Ã© o mesmo para todas as aÃ§Ãµes)
Â  Â  Â  Â  states = np.array([b[0] for b in batch])
Â  Â  Â  Â  next_states = np.array([b[3] for b in batch])
Â  Â  Â  Â  
Â  Â  Â  Â  if states.size == 0 or next_states.size == 0:
Â  Â  Â  Â  Â  Â  return
Â  Â  Â  Â  
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # PrevisÃµes para o prÃ³ximo estado (Q-learning)
Â  Â  Â  Â  Â  Â  q_next = self.target_model.predict(next_states, verbose=0)
Â  Â  Â  Â  Â  Â  # PrevisÃµes para o estado atual
Â  Â  Â  Â  Â  Â  q_curr = self.model.predict(states, verbose=0)
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  return

Â  Â  Â  Â  # Construir os lotes de treinamento
Â  Â  Â  Â  X, Y = [], []
Â  Â  Â  Â  for i, (state, actions, reward, next_state, done) in enumerate(batch):
Â  Â  Â  Â  Â  Â  target = q_curr[i].copy()
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # O loop principal para ajustar o Q-value de cada aÃ§Ã£o do lote
Â  Â  Â  Â  Â  Â  for action in actions:
Â  Â  Â  Â  Â  Â  Â  Â  if done:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Se o jogo terminou, a recompensa Ã© o valor final
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  target[action] = reward
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Caso contrÃ¡rio, aplica a equaÃ§Ã£o de Bellman
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  next_q = q_next[i] if i < len(q_next) else np.zeros(self.action_size)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  target[action] = reward + self.gamma * np.max(next_q)
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  X.append(state)
Â  Â  Â  Â  Â  Â  Y.append(target)
Â  Â  Â  Â  
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  self.model.fit(np.array(X), np.array(Y), epochs=1, verbose=0)
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logger.error(f"Erro no treinamento do DQN: {e}")
Â  Â  Â  Â  Â  Â  pass
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  if self.epsilon > self.epsilon_min:
Â  Â  Â  Â  Â  Â  self.epsilon *= self.epsilon_decay

Â  Â  def load(self, path):
Â  Â  Â  Â  self.model.load_weights(path)
Â  Â  Â  Â  self.update_target()

Â  Â  def save(self, path):
Â  Â  Â  Â  self.model.save_weights(path)

# --- Neighbors ---
def optimal_neighbors(number, max_neighbors=2):
Â  Â  """
Â  Â  Retorna lista de vizinhos (Ã  esquerda e direita alternados) da roda.
Â  Â  """
Â  Â  if number not in WHEEL_ORDER:
Â  Â  Â  Â  return []
Â  Â  idx = WHEEL_ORDER.index(number)
Â  Â  neigh = []
Â  Â  for i in range(1, max_neighbors + 1):
Â  Â  Â  Â  neigh.append(WHEEL_ORDER[(idx - i) % NUM_TOTAL])
Â  Â  Â  Â  neigh.append(WHEEL_ORDER[(idx + i) % NUM_TOTAL])
Â  Â  return list(dict.fromkeys(neigh))
Â  Â  
# =========================================================================
# NOVAS FUNÃ‡Ã•ES: TABELA DE NÃšMEROS QUE SE PUXAM E VIZINHOS DINÃ‚MICOS
# =========================================================================
def update_pull_table(history):
Â  Â  """
Â  Â  Atualiza a tabela de co-ocorrÃªncia a cada novo nÃºmero.
Â  Â  """
Â  Â  if len(history) < 2:
Â  Â  Â  Â  return
Â  Â  
Â  Â  prev_num = str(history[-2])
Â  Â  current_num = str(history[-1])
Â  Â  
Â  Â  if prev_num in st.session_state.pull_table and current_num in st.session_state.pull_table[prev_num]:
Â  Â  Â  Â  st.session_state.pull_table[prev_num][current_num] += 1
Â  Â  Â  Â  
def get_pull_weights(last_number, pull_table, alpha=0.5):
Â  Â  """
Â  Â  Calcula os pesos de 'pull' baseados na tabela de co-ocorrÃªncia.
Â  Â  """
Â  Â  if str(last_number) not in pull_table:
Â  Â  Â  Â  return np.ones(NUM_TOTAL)
Â  Â  
Â  Â  counts = pull_table[str(last_number)]
Â  Â  total_count = sum(counts.values())
Â  Â  
Â  Â  if total_count == 0:
Â  Â  Â  Â  return np.ones(NUM_TOTAL)
Â  Â  Â  Â  
Â  Â  weights = np.array([counts.get(str(i), 0) / total_count for i in range(NUM_TOTAL)])
Â  Â  
Â  Â  # Aplica um suavizador (smoothing) para evitar zero weights
Â  Â  weights = weights + alpha
Â  Â  
Â  Â  return weights / weights.sum()

def get_smart_neighbor_suggestions(top_k_predictions, pull_table, n_neighbors=3):
Â  Â  """
Â  Â  Sugere vizinhos dinÃ¢micos com base nos nÃºmeros que mais se 'puxam'.
Â  Â  """
Â  Â  suggestions = set(top_k_predictions)
Â  Â  
Â  Â  if len(st.session_state.history) > 0:
Â  Â  Â  Â  last_num = st.session_state.history[-1]
Â  Â  Â  Â  
Â  Â  Â  Â  # 1. Sugere os vizinhos fÃ­sicos mais prÃ³ximos do Ãºltimo nÃºmero
Â  Â  Â  Â  suggestions.update(optimal_neighbors(last_num, max_neighbors=1))
Â  Â  Â  Â  
Â  Â  Â  Â  # 2. Sugere os vizinhos da tabela de pull dos top K
Â  Â  Â  Â  for num in top_k_predictions:
Â  Â  Â  Â  Â  Â  if str(num) in pull_table:
Â  Â  Â  Â  Â  Â  Â  Â  counts = pull_table[str(num)]
Â  Â  Â  Â  Â  Â  Â  Â  # Seleciona os N vizinhos com maior contagem
Â  Â  Â  Â  Â  Â  Â  Â  top_neighbors_pull = sorted(counts.items(), key=lambda item: item[1], reverse=True)[:n_neighbors]
Â  Â  Â  Â  Â  Â  Â  Â  suggestions.update([int(n) for n, count in top_neighbors_pull])
Â  Â  
Â  Â  return sorted(list(suggestions))

# =========================
# RECOMPENSA FOCADA E SIMPLIFICADA
# =========================
def compute_reward(action_numbers, outcome_number, bet_amount=BET_AMOUNT,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â max_neighbors_for_reward=NEIGHBOR_RADIUS_FOR_REWARD):
Â  Â  """
Â  Â  Recompensa focada: apenas premia acerto de nÃºmero ou vizinho.
Â  Â  Perda em todos os outros casos.
Â  Â  """
Â  Â  reward = 0.0
Â  Â  action_numbers = set([a for a in action_numbers if 0 <= a <= 36])

Â  Â  # 1. Acerto Exato (maior recompensa e prioridade)
Â  Â  if outcome_number in action_numbers:
Â  Â  Â  Â  reward = REWARD_EXACT
Â  Â  # 2. Acerto Vizinho (recompensa menor)
Â  Â  else:
Â  Â  Â  Â  all_neighbors = set()
Â  Â  Â  Â  for a in action_numbers:
Â  Â  Â  Â  Â  Â  all_neighbors.update(optimal_neighbors(a, max_neighbors=max_neighbors_for_reward))
Â  Â  Â  Â  
Â  Â  Â  Â  if outcome_number in all_neighbors:
Â  Â  Â  Â  Â  Â  reward = REWARD_NEIGHBOR
Â  Â  Â  Â  # 3. Perda total (penalidade)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  reward = REWARD_LOSS

Â  Â  return reward * bet_amount

# --- PREDICTION POSTPROCESSING ---
def predict_next_numbers(model, history, pull_table, top_k=3):
Â  Â  if history is None or len(history) < SEQUENCE_LEN or model is None:
Â  Â  Â  Â  return []
Â  Â  try:
Â  Â  Â  Â  seq_one_hot = sequence_to_one_hot(history).reshape(1, SEQUENCE_LEN, NUM_TOTAL)
Â  Â  Â  Â  feat = np.array([get_advanced_features(history[-SEQUENCE_LEN:])])
Â  Â  Â  Â  raw = model.predict([seq_one_hot, feat], verbose=0)
Â  Â  Â  Â  # raw -> [num_probs, color_probs, dozen_probs]
Â  Â  Â  Â  if isinstance(raw, list) and len(raw) == 3:
Â  Â  Â  Â  Â  Â  num_probs = raw[0][0]
Â  Â  Â  Â  Â  Â  color_probs = raw[1][0]
Â  Â  Â  Â  Â  Â  dozen_probs = raw[2][0]
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  num_probs = np.array(raw)[0]
Â  Â  Â  Â  Â  Â  color_probs = np.array([0.0, 0.0, 0.0])
Â  Â  Â  Â  Â  Â  dozen_probs = np.array([0.0, 0.0, 0.0, 0.0])
Â  Â  except Exception:
Â  Â  Â  Â  return []

Â  Â  # temperature + heurÃ­sticas
Â  Â  temperature = 0.8
Â  Â  adjusted = np.log(num_probs + 1e-12) / temperature
Â  Â  adjusted = np.exp(adjusted)
Â  Â  adjusted /= adjusted.sum()

Â  Â  weighted = []
Â  Â  freq_counter = Counter(history[-100:])
Â  Â  last_num = history[-1] if len(history) > 0 else None
Â  Â  
Â  Â  # NOVOS PESOS DA TABELA DE PUXAR
Â  Â  pull_weights = get_pull_weights(last_num, pull_table, alpha=1.0)
Â  Â  
Â  Â  for num in range(NUM_TOTAL):
Â  Â  Â  Â  freq_factor = 1 + np.exp(freq_counter.get(num, 0) / 3 - 1)
Â  Â  Â  Â  if last_num in WHEEL_ORDER:
Â  Â  Â  Â  Â  Â  dist = WHEEL_DISTANCE[WHEEL_ORDER.index(last_num)][WHEEL_ORDER.index(num)]
Â  Â  Â  Â  Â  Â  distance_factor = max(0.1, 2.5 - (dist / 12.0))
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  distance_factor = 1.0
Â  Â  Â  Â  momentum = sum(1 for i in range(1,4) if len(history)>=i and history[-i] == num)
Â  Â  Â  Â  momentum_factor = 1 + momentum*0.25
Â  Â  Â  Â  
Â  Â  Â  Â  # PonderaÃ§Ã£o combinada com o novo fator de "pull"
Â  Â  Â  Â  weighted.append(adjusted[num] * freq_factor * distance_factor * momentum_factor * pull_weights[num])
Â  Â  Â  Â  
Â  Â  weighted = np.array(weighted)
Â  Â  if weighted.sum() == 0:
Â  Â  Â  Â  return []
Â  Â  weighted /= weighted.sum()

Â  Â  top_indices = list(np.argsort(weighted)[-top_k:][::-1])
Â  Â  color_pred = int(np.argmax(color_probs))
Â  Â  dozen_pred = int(np.argmax(dozen_probs))
Â  Â  return {
Â  Â  Â  Â  'top_numbers': [(int(i), float(weighted[i])) for i in top_indices],
Â  Â  Â  Â  'num_probs': num_probs,
Â  Â  Â  Â  'color_probs': color_probs,
Â  Â  Â  Â  'dozen_pred': dozen_pred,
Â  Â  Â  Â  'dozen_probs': dozen_probs,
Â  Â  Â  Â  'color_pred': color_pred
Â  Â  }

# =========================
# LSTM: construÃ§Ã£o de dataset e treino recente
# =========================
def build_lstm_supervised_from_history(history):
Â  Â  """
Â  Â  ConstrÃ³i X_seq, X_feat, y_num, y_color, y_dozen a partir do histÃ³rico.
Â  Â  Retorna arrays numpy prontos para treino.
Â  Â  """
Â  Â  X_seq, X_feat, y_num, y_color, y_dozen = [], [], [], [], []
Â  Â  if len(history) <= SEQUENCE_LEN:
Â  Â  Â  Â  return None

Â  Â  start_idx = max(0, len(history) - (SEQUENCE_LEN + 1) - LSTM_RECENT_WINDOWS)
Â  Â  for i in range(start_idx, len(history) - SEQUENCE_LEN - 1):
Â  Â  Â  Â  seq_slice = history[i:i+SEQUENCE_LEN]
Â  Â  Â  Â  target = history[i+SEQUENCE_LEN]

Â  Â  Â  Â  X_seq.append(sequence_to_one_hot(seq_slice))
Â  Â  Â  Â  X_feat.append(get_advanced_features(seq_slice))

Â  Â  Â  Â  if target in WHEEL_ORDER:
Â  Â  Â  Â  Â  Â  pos = WHEEL_ORDER.index(target)
Â  Â  Â  Â  Â  Â  y_num.append(to_categorical(pos, NUM_TOTAL))
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  y_num.append(np.zeros(NUM_TOTAL))

Â  Â  Â  Â  color_label = number_to_color(target)
Â  Â  Â  Â  y_color.append(to_categorical(color_label, 3))

Â  Â  Â  Â  dozen_label = number_to_dozen(target)
Â  Â  Â  Â  y_dozen.append(to_categorical(dozen_label, 4))

Â  Â  if len(X_seq) == 0:
Â  Â  Â  Â  return None

Â  Â  X_seq = np.array(X_seq)
Â  Â  X_feat = np.array(X_feat)
Â  Â  y_num = np.array(y_num)
Â  Â  y_color = np.array(y_color)
Â  Â  y_dozen = np.array(y_dozen)
Â  Â  return X_seq, X_feat, y_num, y_color, y_dozen

def train_lstm_on_recent_minibatch(model, history):
Â  Â  """
Â  Â  Treina o LSTM usando amostras aleatÃ³rias de janelas recentes,
Â  Â  evitando reprocessar todo histÃ³rico em cada passo.
Â  Â  """
Â  Â  data = build_lstm_supervised_from_history(history)
Â  Â  if data is None:
Â  Â  Â  Â  return
Â  Â  X_seq, X_feat, y_num, y_color, y_dozen = data
Â  Â  n = len(X_seq)
Â  Â  if n == 0:
Â  Â  Â  Â  return

Â  Â  # Amostragem aleatÃ³ria sem reposiÃ§Ã£o
Â  Â  k = min(n, LSTM_BATCH_SAMPLES)
Â  Â  idx = np.random.choice(n, k, replace=False)
Â  Â  try:
Â  Â  Â  Â  model.fit([X_seq[idx], X_feat[idx]],
Â  Â  Â  Â  Â  Â  Â  Â  Â  [y_num[idx], y_color[idx], y_dozen[idx]],
Â  Â  Â  Â  Â  Â  Â  Â  Â  epochs=LSTM_EPOCHS_PER_STEP,
Â  Â  Â  Â  Â  Â  Â  Â  Â  batch_size=LSTM_BATCH_SIZE,
Â  Â  Â  Â  Â  Â  Â  Â  Â  verbose=0)
Â  Â  Â  Â  logger.info(f"LSTM mini-train: {k} amostras de {n} ({LSTM_EPOCHS_PER_STEP} Ã©pocas).")
Â  Â  except Exception as e:
Â  Â  Â  Â  logger.error(f"Erro no treinamento LSTM: {e}")

# --- UI ---
st.set_page_config(layout="centered")
st.title("ðŸ”¥ ROULETTE AI - LSTM multi-saÃ­da + DQN (REVISADO + REWARD / TREINO RECENTE)")

st.markdown("### Inserir histÃ³rico manualmente (ex: 0,32,15,19,4,21)")

# 1) Garantir chaves no session_state
if 'input_bulk' not in st.session_state:
Â  Â  st.session_state.input_bulk = ""
if 'clear_input_bulk' not in st.session_state:
Â  Â  st.session_state.clear_input_bulk = False

# 2) APLICAR LIMPEZA ANTES DE CRIAR O WIDGET
if st.session_state.clear_input_bulk:
Â  Â  st.session_state.input_bulk = ""
Â  Â  st.session_state.clear_input_bulk = False

# 3) Criar o text_area
input_bulk = st.text_area("Cole nÃºmeros separados por vÃ­rgula", key="input_bulk")

# 4) BotÃ£o para adicionar histÃ³rico
if st.button("Adicionar histÃ³rico"):
Â  Â  if st.session_state.input_bulk and st.session_state.input_bulk.strip():
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  new_nums = [
Â  Â  Â  Â  Â  Â  Â  Â  int(x.strip())
Â  Â  Â  Â  Â  Â  Â  Â  for x in st.session_state.input_bulk.split(",")
Â  Â  Â  Â  Â  Â  Â  Â  if x.strip().isdigit() and 0 <= int(x.strip()) <= 36
Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # Atualiza a pull_table para os novos nÃºmeros
Â  Â  Â  Â  Â  Â  if st.session_state.history:
Â  Â  Â  Â  Â  Â  Â  Â  prev_num = st.session_state.history[-1]
Â  Â  Â  Â  Â  Â  Â  Â  for num in new_nums:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.pull_table[str(prev_num)][str(num)] += 1
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prev_num = num
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  st.session_state.history.extend(new_nums)
Â  Â  Â  Â  Â  Â  st.success(f"Adicionados {len(new_nums)} nÃºmeros ao histÃ³rico.")
Â  Â  Â  Â  Â  Â  st.session_state.clear_input_bulk = True
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.error(f"Erro ao processar nÃºmeros: {e}")
Â  Â  else:
Â  Â  Â  Â  st.warning("Insira nÃºmeros vÃ¡lidos para adicionar.")

st.markdown("---")

with st.form("num_form", clear_on_submit=True):
Â  Â  num_input = st.number_input("Digite o Ãºltimo nÃºmero (0-36):", min_value=0, max_value=36, step=1, key="current_number")
Â  Â  submitted = st.form_submit_button("Enviar")
Â  Â  if submitted:
Â  Â  Â  Â  st.session_state.last_input = int(num_input)

if st.session_state.last_input is not None:
Â  Â  try:
Â  Â  Â  Â  num = int(st.session_state.last_input)
Â  Â  Â  Â  
Â  Â  Â  Â  # ATUALIZA A TABELA DE PUXAR ANTES DE ADICIONAR O NOVO NÃšMERO
Â  Â  Â  Â  if len(st.session_state.history) > 0:
Â  Â  Â  Â  Â  Â  update_pull_table(st.session_state.history + [num])
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  st.session_state.history.append(num)
Â  Â  Â  Â  logger.info(f"NÃºmero novo inserido pelo usuÃ¡rio: {num}")
Â  Â  Â  Â  st.session_state.last_input = None

Â  Â  Â  Â  state_example = sequence_to_state(st.session_state.history, st.session_state.model)
Â  Â  Â  Â  if state_example is not None and (st.session_state.dqn_agent is None):
Â  Â  Â  Â  Â  Â  st.session_state.dqn_agent = DQNAgent(state_size=state_example.shape[0], action_size=NUM_TOTAL)
Â  Â  Â  Â  Â  Â  logger.info("Agente DQN criado")

Â  Â  Â  Â  if st.session_state.prev_state is not None and st.session_state.prev_actions is not None:
Â  Â  Â  Â  Â  Â  agent = st.session_state.dqn_agent
Â  Â  Â  Â  Â  Â  reward = compute_reward(st.session_state.prev_actions, num, bet_amount=BET_AMOUNT,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  max_neighbors_for_reward=NEIGHBOR_RADIUS_FOR_REWARD)
Â  Â  Â  Â  Â  Â  next_state = sequence_to_state(st.session_state.history, st.session_state.model)

Â  Â  Â  Â  Â  Â  if agent is not None:
Â  Â  Â  Â  Â  Â  Â  Â  agent.remember(st.session_state.prev_state, st.session_state.prev_actions, reward, next_state, False)
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Memorizado: aÃ§Ãµes={st.session_state.prev_actions}, resultado={num}, recompensa={reward}")

Â  Â  Â  Â  Â  Â  st.session_state.stats['bets'] += 1
Â  Â  Â  Â  Â  Â  st.session_state.stats['profit'] += reward
Â  Â  Â  Â  Â  Â  if reward > 0:
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.stats['wins'] += 1
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.stats['streak'] += 1
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.stats['max_streak'] = max(st.session_state.stats['max_streak'], st.session_state.stats['streak'])
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.stats['streak'] = 0

Â  Â  Â  Â  Â  Â  st.session_state.step_count += 1
Â  Â  Â  Â  Â  Â  if agent is not None and st.session_state.step_count % DQN_TRAIN_EVERY == 0:
Â  Â  Â  Â  Â  Â  Â  Â  agent.replay(REPLAY_BATCH)
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"DQN treinado no passo {st.session_state.step_count}")
Â  Â  Â  Â  Â  Â  if agent is not None and st.session_state.step_count % TARGET_UPDATE_FREQ == 0:
Â  Â  Â  Â  Â  Â  Â  Â  agent.update_target()
Â  Â  Â  Â  Â  Â  Â  Â  logger.info(f"Target DQN atualizado no passo {st.session_state.step_count}")

Â  Â  Â  Â  if st.session_state.model is None and len(st.session_state.history) >= SEQUENCE_LEN*2:
Â  Â  Â  Â  Â  Â  st.session_state.model = build_deep_learning_model()
Â  Â  Â  Â  Â  Â  logger.info("Modelo LSTM criado")

Â  Â  Â  Â  if st.session_state.model is not None and len(st.session_state.history) > SEQUENCE_LEN*2:
Â  Â  Â  Â  Â  Â  with st.spinner("Treinando LSTM com mini-batches recentes..."):
Â  Â  Â  Â  Â  Â  Â  Â  train_lstm_on_recent_minibatch(st.session_state.model, st.session_state.history)
Â  Â  Â  Â  Â  Â  st.session_state.prev_state = sequence_to_state(st.session_state.history, st.session_state.model)
Â  Â  Â  Â  Â  Â  pred_info = predict_next_numbers(st.session_state.model, st.session_state.history, st.session_state.pull_table, top_k=3)

Â  Â  Â  Â  Â  Â  if pred_info:
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("ðŸŽ¯ PrevisÃµes (LSTM + pÃ³s-processamento)")
Â  Â  Â  Â  Â  Â  Â  Â  for n, conf in pred_info['top_numbers']:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"NÃºmero: **{n}** â€” Prob: {conf:.2f}")

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("SugestÃµes de Vizinhos DinÃ¢micos")
Â  Â  Â  Â  Â  Â  Â  Â  top_k_numbers = [n for n, conf in pred_info['top_numbers']]
Â  Â  Â  Â  Â  Â  Â  Â  suggestions = get_smart_neighbor_suggestions(top_k_numbers, st.session_state.pull_table, n_neighbors=2)
Â  Â  Â  Â  Â  Â  Â  Â  if suggestions:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"Sugeridos: **{', '.join(map(str, suggestions))}**")

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("EstatÃ­sticas de Desempenho (DQN)")
Â  Â  Â  Â  Â  Â  Â  Â  col1, col2, col3 = st.columns(3)
Â  Â  Â  Â  Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric(label="Total de Apostas", value=st.session_state.stats['bets'])
Â  Â  Â  Â  Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric(label="VitÃ³rias", value=st.session_state.stats['wins'])
Â  Â  Â  Â  Â  Â  Â  Â  with col3:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric(label="Lucro/PrejuÃ­zo", value=f"R$ {st.session_state.stats['profit']:.2f}")

Â  Â  Â  Â  Â  Â  Â  Â  # ================================================
Â  Â  Â  Â  Â  Â  Â  Â  # LINHAS REMOVIDAS PARA OCULTAR A TABELA DE CO-OCORRÃŠNCIA
Â  Â  Â  Â  Â  Â  Â  Â  # O cÃ³digo abaixo foi comentado para nÃ£o exibir a tabela
Â  Â  Â  Â  Â  Â  Â  Â  # ================================================
Â  Â  Â  Â  Â  Â  Â  Â  # st.markdown("---")
Â  Â  Â  Â  Â  Â  Â  Â  # st.subheader("Tabela de Co-ocorrÃªncia ('NÃºmeros que se Puxam')")
Â  Â  Â  Â  Â  Â  Â  Â  # pull_df = pd.DataFrame(st.session_state.pull_table).fillna(0).astype(int)
Â  Â  Â  Â  Â  Â  Â  Â  # st.dataframe(pull_df)
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  # ================================================

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("HistÃ³rico Recente")
Â  Â  Â  Â  Â  Â  Â  Â  st.write(st.session_state.history[-20:])
Â  Â  Â  Â  
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Erro ao processar o nÃºmero: {e}")
Â  Â  Â  Â  logger.error(f"Erro fatal: {e}")
Â  Â  Â  Â  st.session_state.last_input = None
Â  Â  Â  Â  st.rerun()

